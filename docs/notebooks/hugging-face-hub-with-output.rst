ü§óHuggingFaceModelHubwithOpenVINO‚Ñ¢
=======================================

TheHuggingFace(HF)`ModelHub<https://huggingface.co/models>`__isa
centralrepositoryforpre-traineddeeplearningmodels.Itallows
explorationandprovidesaccesstothousandsofmodelsforawiderange
oftasks,includingtextclassification,questionanswering,andimage
classification.HuggingFaceprovidesPythonpackagesthatserveasAPIs
andtoolstoeasilydownloadandfinetunestate-of-the-artpretrained
models,namely
`transformers<https://github.com/huggingface/transformers>`__and
`diffusers<https://github.com/huggingface/diffusers>`__packages.

|image0|

Throughoutthisnotebookwewilllearn:1.HowtoloadaHFpipeline
usingthe``transformers``packageandthenconvertittoOpenVINO.2.
HowtoloadthesamepipelineusingOptimumIntelpackage.

Tableofcontents:
^^^^^^^^^^^^^^^^^^

-`ConvertingaModelfromtheHFTransformers
Package<#converting-a-model-from-the-hf-transformers-package>`__

-`InstallingRequirements<#installing-requirements>`__
-`Imports<#imports>`__
-`InitializingaModelUsingtheHFTransformers
Package<#initializing-a-model-using-the-hf-transformers-package>`__
-`OriginalModelinference<#original-model-inference>`__
-`ConvertingtheModeltoOpenVINOIR
format<#converting-the-model-to-openvino-ir-format>`__
-`ConvertedModelInference<#converted-model-inference>`__

-`ConvertingaModelUsingtheOptimumIntel
Package<#converting-a-model-using-the-optimum-intel-package>`__

-`InstallRequirementsfor
Optimum<#install-requirements-for-optimum>`__
-`ImportOptimum<#import-optimum>`__
-`InitializeandConverttheModelAutomaticallyusingOVModel
class<#initialize-and-convert-the-model-automatically-using-ovmodel-class>`__
-`ConvertmodelusingOptimumCLI
interface<#convert-model-using-optimum-cli-interface>`__
-`TheOptimumModelInference<#the-optimum-model-inference>`__

..|image0|image::https://github.com/huggingface/optimum-intel/raw/main/readme_logo.png

ConvertingaModelfromtheHFTransformersPackage
---------------------------------------------------

`backtotop‚¨ÜÔ∏è<#table-of-contents>`__

HuggingFacetransformerspackageprovidesAPIforinitializingamodel
andloadingasetofpre-trainedweightsusingthemodeltexthandle.
Discoveringadesiredmodelnameisstraightforwardwith`HFwebsite‚Äôs
Modelspage<https://huggingface.co/models>`__,onecanchooseamodel
solvingaparticularmachinelearningproblemandevensortthemodels
bypopularityandnovelty.

InstallingRequirements
~~~~~~~~~~~~~~~~~~~~~~~

`backtotop‚¨ÜÔ∏è<#table-of-contents>`__

..code::ipython3

%pipinstall-q--extra-index-urlhttps://download.pytorch.org/whl/cpu"transformers>=4.33.0""torch>=2.1.0"
%pipinstall-qipywidgets
%pipinstall-q"openvino>=2023.1.0"


..parsed-literal::

Note:youmayneedtorestartthekerneltouseupdatedpackages.
Note:youmayneedtorestartthekerneltouseupdatedpackages.
Note:youmayneedtorestartthekerneltouseupdatedpackages.


Imports
~~~~~~~

`backtotop‚¨ÜÔ∏è<#table-of-contents>`__

..code::ipython3

frompathlibimportPath

importnumpyasnp
importtorch

fromtransformersimportAutoModelForSequenceClassification
fromtransformersimportAutoTokenizer

InitializingaModelUsingtheHFTransformersPackage
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

`backtotop‚¨ÜÔ∏è<#table-of-contents>`__

Wewilluse`robertatextsentiment
classification<https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest>`__
modelinourexample,itisatransformer-basedencodermodelpretrained
inaspecialway,pleaserefertothemodelcardtolearnmore.

Followingtheinstructionsonthemodelpage,weuse
``AutoModelForSequenceClassification``toinitializethemodeland
performinferencewithit.TofindmoreinformationonHFpipelinesand
modelinitializationpleasereferto`HF
tutorials<https://huggingface.co/learn/nlp-course/chapter2/2?fw=pt#behind-the-pipeline>`__.

..code::ipython3

MODEL="cardiffnlp/twitter-roberta-base-sentiment-latest"

tokenizer=AutoTokenizer.from_pretrained(MODEL,return_dict=True)

#Thetorchscript=Trueflagisusedtoensurethemodeloutputsaretuples
#insteadofModelOutput(whichcausesJITerrors).
model=AutoModelForSequenceClassification.from_pretrained(MODEL,torchscript=True)


..parsed-literal::

Someweightsofthemodelcheckpointatcardiffnlp/twitter-roberta-base-sentiment-latestwerenotusedwheninitializingRobertaForSequenceClassification:['roberta.pooler.dense.bias','roberta.pooler.dense.weight']
-ThisISexpectedifyouareinitializingRobertaForSequenceClassificationfromthecheckpointofamodeltrainedonanothertaskorwithanotherarchitecture(e.g.initializingaBertForSequenceClassificationmodelfromaBertForPreTrainingmodel).
-ThisISNOTexpectedifyouareinitializingRobertaForSequenceClassificationfromthecheckpointofamodelthatyouexpecttobeexactlyidentical(initializingaBertForSequenceClassificationmodelfromaBertForSequenceClassificationmodel).


OriginalModelinference
~~~~~~~~~~~~~~~~~~~~~~~~

`backtotop‚¨ÜÔ∏è<#table-of-contents>`__

Let‚Äôsdoaclassificationofasimplepromptbelow.

..code::ipython3

text="HFmodelsrunperfectlywithOpenVINO!"

encoded_input=tokenizer(text,return_tensors="pt")
output=model(**encoded_input)
scores=output[0][0]
scores=torch.softmax(scores,dim=0).numpy(force=True)


defprint_prediction(scores):
fori,descending_indexinenumerate(scores.argsort()[::-1]):
label=model.config.id2label[descending_index]
score=np.round(float(scores[descending_index]),4)
print(f"{i+1}){label}{score}")


print_prediction(scores)


..parsed-literal::

1)positive0.9485
2)neutral0.0484
3)negative0.0031


ConvertingtheModeltoOpenVINOIRformat
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

`backtotop‚¨ÜÔ∏è<#table-of-contents>`__WeusetheOpenVINO`Model
conversion
API<https://docs.openvino.ai/2024/openvino-workflow/model-preparation.html#convert-a-model-with-python-convert-model>`__
toconvertthemodel(thisoneisimplementedinPyTorch)toOpenVINO
IntermediateRepresentation(IR).

Notehowwereuseourreal``encoded_input``,passingittothe
``ov.convert_model``function.Itwillbeusedformodeltracing.

..code::ipython3

importopenvinoasov

save_model_path=Path("./models/model.xml")

ifnotsave_model_path.exists():
ov_model=ov.convert_model(model,example_input=dict(encoded_input))
ov.save_model(ov_model,save_model_path)


..parsed-literal::

/opt/home/k8sworker/ci-ai/cibuilds/ov-notebook/OVNotebookOps-727/.workspace/scm/ov-notebook/.venv/lib/python3.8/site-packages/transformers/modeling_utils.py:4565:FutureWarning:`_is_quantized_training_enabled`isgoingtobedeprecatedintransformers4.39.0.Pleaseuse`model.hf_quantizer.is_trainable`instead
warnings.warn(


ConvertedModelInference
~~~~~~~~~~~~~~~~~~~~~~~~~

`backtotop‚¨ÜÔ∏è<#table-of-contents>`__

First,wepickadevicetodothemodelinference

..code::ipython3

importipywidgetsaswidgets

core=ov.Core()

device=widgets.Dropdown(
options=core.available_devices+["AUTO"],
value="AUTO",
description="Device:",
disabled=False,
)

device




..parsed-literal::

Dropdown(description='Device:',index=1,options=('CPU','AUTO'),value='AUTO')



OpenVINOmodelIRmustbecompiledforaspecificdevicepriortothe
modelinference.

..code::ipython3

compiled_model=core.compile_model(save_model_path,device.value)

#Compiledmodelcallisperformedusingthesameparametersasfortheoriginalmodel
scores_ov=compiled_model(encoded_input.data)[0]

scores_ov=torch.softmax(torch.tensor(scores_ov[0]),dim=0).detach().numpy()

print_prediction(scores_ov)


..parsed-literal::

1)positive0.9483
2)neutral0.0485
3)negative0.0031


Notethepredictionoftheconvertedmodelmatchexactlytheoneofthe
originalmodel.

Thisisarathersimpleexampleasthepipelineincludesjustone
encodermodel.Contemporarystateoftheartpipelinesoftenconsistof
severalmodel,feelfreetoexploreotherOpenVINOtutorials:1.`Stable
Diffusionv2<../stable-diffusion-v2>`__2.`Zero-shotImage
ClassificationwithOpenAI
CLIP<../clip-zero-shot-image-classification>`__3.`ControllableMusic
GenerationwithMusicGen<../music-generation>`__

Theworkflowforthe``diffusers``packageisexactlythesame.The
firstexampleinthelistabovereliesonthe``diffusers``.

ConvertingaModelUsingtheOptimumIntelPackage
--------------------------------------------------

`backtotop‚¨ÜÔ∏è<#table-of-contents>`__

ü§óOptimumIntelistheinterfacebetweentheü§óTransformersand
Diffuserslibrariesandthedifferenttoolsandlibrariesprovidedby
Inteltoaccelerateend-to-endpipelinesonIntelarchitectures.

Amongotherusecases,OptimumIntelprovidesasimpleinterfaceto
optimizeyourTransformersandDiffusersmodels,convertthemtothe
OpenVINOIntermediateRepresentation(IR)formatandruninferenceusing
OpenVINORuntime.

InstallRequirementsforOptimum
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

`backtotop‚¨ÜÔ∏è<#table-of-contents>`__

..code::ipython3

%pipinstall-q"git+https://github.com/huggingface/optimum-intel.git"onnx


..parsed-literal::

huggingface/tokenizers:Thecurrentprocessjustgotforked,afterparallelismhasalreadybeenused.Disablingparallelismtoavoiddeadlocks...
Todisablethiswarning,youcaneither:
	-Avoidusing`tokenizers`beforetheforkifpossible
	-ExplicitlysettheenvironmentvariableTOKENIZERS_PARALLELISM=(true|false)


..parsed-literal::

Note:youmayneedtorestartthekerneltouseupdatedpackages.


ImportOptimum
~~~~~~~~~~~~~~

`backtotop‚¨ÜÔ∏è<#table-of-contents>`__

DocumentationforOptimumIntelstates:>Youcannoweasilyperform
inferencewithOpenVINORuntimeonavarietyofIntelprocessors(see
thefulllistofsupporteddevices).Forthat,justreplacethe
``AutoModelForXxx``classwiththecorresponding``OVModelForXxx``
class.

Youcanfindmoreinformationin`OptimumIntel
documentation<https://huggingface.co/docs/optimum/intel/inference>`__.

..code::ipython3

fromoptimum.intel.openvinoimportOVModelForSequenceClassification


..parsed-literal::

huggingface/tokenizers:Thecurrentprocessjustgotforked,afterparallelismhasalreadybeenused.Disablingparallelismtoavoiddeadlocks...
Todisablethiswarning,youcaneither:
	-Avoidusing`tokenizers`beforetheforkifpossible
	-ExplicitlysettheenvironmentvariableTOKENIZERS_PARALLELISM=(true|false)
2024-07-1300:35:27.817822:Itensorflow/core/util/port.cc:110]oneDNNcustomoperationsareon.Youmayseeslightlydifferentnumericalresultsduetofloating-pointround-offerrorsfromdifferentcomputationorders.Toturnthemoff,settheenvironmentvariable`TF_ENABLE_ONEDNN_OPTS=0`.
2024-07-1300:35:27.853673:Itensorflow/core/platform/cpu_feature_guard.cc:182]ThisTensorFlowbinaryisoptimizedtouseavailableCPUinstructionsinperformance-criticaloperations.
Toenablethefollowinginstructions:AVX2AVX512FAVX512_VNNIFMA,inotheroperations,rebuildTensorFlowwiththeappropriatecompilerflags.
2024-07-1300:35:28.470157:Wtensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38]TF-TRTWarning:CouldnotfindTensorRT
/opt/home/k8sworker/ci-ai/cibuilds/ov-notebook/OVNotebookOps-727/.workspace/scm/ov-notebook/.venv/lib/python3.8/site-packages/diffusers/utils/outputs.py:63:UserWarning:torch.utils._pytree._register_pytree_nodeisdeprecated.Pleaseusetorch.utils._pytree.register_pytree_nodeinstead.
torch.utils._pytree._register_pytree_node(


InitializeandConverttheModelAutomaticallyusingOVModelclass
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

`backtotop‚¨ÜÔ∏è<#table-of-contents>`__

ToloadaTransformersmodelandconvertittotheOpenVINOformaton
thefly,youcanset``export=True``whenloadingyourmodel.Themodel
canbesavedinOpenVINOformatusing``save_pretrained``methodand
specifyingadirectoryforstoringthemodelasanargument.Forthe
nextusage,youcanavoidtheconversionstepandloadthesavedearly
modelfromdiskusing``from_pretrained``methodwithoutexport
specification.Wealsospecified``device``parameterforcompilingthe
modelonthespecificdevice,ifnotprovided,thedefaultdevicewill
beused.Thedevicecanbechangedlaterinruntimeusing
``model.to(device)``,pleasenotethatitmayrequiresometimefor
modelcompilationonanewlyselecteddevice.Insomecases,itcanbe
usefultoseparatemodelinitializationandcompilation,forexample,if
youwanttoreshapethemodelusing``reshape``method,youcanpostpone
compilation,providingtheparameter``compile=False``into
``from_pretrained``method,compilationcanbeperformedmanuallyusing
``compile``methodorwillbeperformedautomaticallyduringfirst
inferencerun.

..code::ipython3

model=OVModelForSequenceClassification.from_pretrained(MODEL,export=True,device=device.value)

#Thesave_pretrained()methodsavesthemodelweightstoavoidconversiononthenextload.
model.save_pretrained("./models/optimum_model")


..parsed-literal::

Frameworknotspecified.Usingpttoexportthemodel.
Someweightsofthemodelcheckpointatcardiffnlp/twitter-roberta-base-sentiment-latestwerenotusedwheninitializingRobertaForSequenceClassification:['roberta.pooler.dense.bias','roberta.pooler.dense.weight']
-ThisISexpectedifyouareinitializingRobertaForSequenceClassificationfromthecheckpointofamodeltrainedonanothertaskorwithanotherarchitecture(e.g.initializingaBertForSequenceClassificationmodelfromaBertForPreTrainingmodel).
-ThisISNOTexpectedifyouareinitializingRobertaForSequenceClassificationfromthecheckpointofamodelthatyouexpecttobeexactlyidentical(initializingaBertForSequenceClassificationmodelfromaBertForSequenceClassificationmodel).
UsingframeworkPyTorch:2.3.1+cpu
Overriding1configurationitem(s)
	-use_cache->False


..parsed-literal::

WARNING:tensorflow:Pleasefixyourimports.Moduletensorflow.python.training.tracking.basehasbeenmovedtotensorflow.python.trackable.base.Theoldmodulewillbedeletedinversion2.11.


..parsed-literal::

CompilingthemodeltoAUTO...


ConvertmodelusingOptimumCLIinterface
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

`backtotop‚¨ÜÔ∏è<#table-of-contents>`__

Alternatively,youcanusetheOptimumCLIinterfaceforconverting
models(supportedstartingoptimum-intel1.12version).Generalcommand
format:

..code::bash

optimum-cliexportopenvino--model<model_id_or_path>--task<task><output_dir>

wheretaskistasktoexportthemodelfor,ifnotspecified,thetask
willbeauto-inferredbasedonthemodel.Availabletasksdependonthe
model,butareamong:[‚Äòdefault‚Äô,‚Äòfill-mask‚Äô,‚Äòtext-generation‚Äô,
‚Äòtext2text-generation‚Äô,‚Äòtext-classification‚Äô,‚Äòtoken-classification‚Äô,
‚Äòmultiple-choice‚Äô,‚Äòobject-detection‚Äô,‚Äòquestion-answering‚Äô,
‚Äòimage-classification‚Äô,‚Äòimage-segmentation‚Äô,‚Äòmasked-im‚Äô,
‚Äòsemantic-segmentation‚Äô,‚Äòautomatic-speech-recognition‚Äô,
‚Äòaudio-classification‚Äô,‚Äòaudio-frame-classification‚Äô,
‚Äòautomatic-speech-recognition‚Äô,‚Äòaudio-xvector‚Äô,‚Äòimage-to-text‚Äô,
‚Äòstable-diffusion‚Äô,‚Äòzero-shot-object-detection‚Äô].Fordecodermodels,
use``xxx-with-past``toexportthemodelusingpastkeyvaluesinthe
decoder.

YoucanfindamappingbetweentasksandmodelclassesinOptimum
TaskManager
`documentation<https://huggingface.co/docs/optimum/exporters/task_manager>`__.

Additionally,youcanspecifyweightscompression``--fp16``forthe
compressionmodeltoFP16and``--int8``forthecompressionmodelto
INT8.Pleasenote,thatforINT8,itisnecessarytoinstallnncf.

Fulllistofsupportedargumentsavailablevia``--help``

..code::ipython3

!optimum-cliexportopenvino--help


..parsed-literal::

huggingface/tokenizers:Thecurrentprocessjustgotforked,afterparallelismhasalreadybeenused.Disablingparallelismtoavoiddeadlocks...
Todisablethiswarning,youcaneither:
	-Avoidusing`tokenizers`beforetheforkifpossible
	-ExplicitlysettheenvironmentvariableTOKENIZERS_PARALLELISM=(true|false)


..parsed-literal::

2024-07-1300:35:41.047556:Wtensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38]TF-TRTWarning:CouldnotfindTensorRT
usage:optimum-cliexportopenvino[-h]-mMODEL[--taskTASK]
[--framework{pt,tf}][--trust-remote-code]
[--weight-format{fp32,fp16,int8,int4,int4_sym_g128,int4_asym_g128,int4_sym_g64,int4_asym_g64}]
[--library{transformers,diffusers,timm,sentence_transformers}]
[--cache_dirCACHE_DIR]
[--pad-token-idPAD_TOKEN_ID]
[--ratioRATIO][--sym]
[--group-sizeGROUP_SIZE]
[--datasetDATASET][--all-layers][--awq]
[--scale-estimation]
[--sensitivity-metricSENSITIVITY_METRIC]
[--num-samplesNUM_SAMPLES]
[--disable-stateful]
[--disable-convert-tokenizer][--fp16]
[--int8][--convert-tokenizer]
output

optionalarguments:
-h,--helpshowthishelpmessageandexit

Requiredarguments:
-mMODEL,--modelMODEL
ModelIDonhuggingface.coorpathondisktoload
modelfrom.
outputPathindicatingthedirectorywheretostorethe
generatedOVmodel.

Optionalarguments:
--taskTASKThetasktoexportthemodelfor.Ifnotspecified,
thetaskwillbeauto-inferredbasedonthemodel.
Availabletasksdependonthemodel,butareamong:
['text-generation','text-to-audio','conversational',
'fill-mask','audio-classification','token-
classification','zero-shot-object-detection','text-
classification','stable-diffusion-xl','question-
answering','feature-extraction','text2text-
generation','sentence-similarity','image-
segmentation','automatic-speech-recognition','depth-
estimation','image-to-image','image-classification',
'stable-diffusion','audio-frame-classification',
'semantic-segmentation','mask-generation','multiple-
choice','audio-xvector','image-to-text','object-
detection','zero-shot-image-classification','masked-
im'].Fordecodermodels,use`xxx-with-past`to
exportthemodelusingpastkeyvaluesinthedecoder.
--framework{pt,tf}Theframeworktousefortheexport.Ifnotprovided,
willattempttousethelocalcheckpoint'soriginal
frameworkorwhatisavailableintheenvironment.
--trust-remote-codeAllowstousecustomcodeforthemodelinghostedin
themodelrepository.Thisoptionshouldonlybeset
forrepositoriesyoutrustandinwhichyouhaveread
thecode,asitwillexecuteonyourlocalmachine
arbitrarycodepresentinthemodelrepository.
--weight-format{fp32,fp16,int8,int4,int4_sym_g128,int4_asym_g128,int4_sym_g64,int4_asym_g64}
heweightformatoftheexportedmodel.
--library{transformers,diffusers,timm,sentence_transformers}
Thelibraryusedtoloadthemodelbeforeexport.If
notprovided,willattempttoinferthelocal
checkpoint'slibrary
--cache_dirCACHE_DIR
Thepathtoadirectoryinwhichthedownloadedmodel
shouldbecachedifthestandardcacheshouldnotbe
used.
--pad-token-idPAD_TOKEN_ID
Thisisneededbysomemodels,forsometasks.Ifnot
provided,willattempttousethetokenizertoguess
it.
--ratioRATIOAparameterusedwhenapplying4-bitquantizationto
controltheratiobetween4-bitand8-bit
quantization.Ifsetto0.8,80%ofthelayerswillbe
quantizedtoint4while20%willbequantizedtoint8.
Thishelpstoachievebetteraccuracyatthesacrifice
ofthemodelsizeandinferencelatency.Defaultvalue
is1.0.
--symWhethertoapplysymmetricquantization
--group-sizeGROUP_SIZE
Thegroupsizetouseforquantization.Recommended
valueis128and-1usesper-columnquantization.
--datasetDATASETThedatasetusedfordata-awarecompressionor
quantizationwithNNCF.Youcanusetheonefromthe
list['wikitext2','c4','c4-new']forlanguagemodels
or['conceptual_captions','laion/220k-GPT4Vision-
captions-from-LIVIS','laion/filtered-wit']for
diffusionmodels.
--all-layersWhetherembeddingsandlastMatMullayersshouldbe
compressedtoINT4.Ifnotprovidedanweight
compressionisapplied,theyarecompressedtoINT8.
--awqWhethertoapplyAWQalgorithm.AWQimproves
generationqualityofINT4-compressedLLMs,but
requiresadditionaltimefortuningweightsona
calibrationdataset.TorunAWQ,pleasealsoprovidea
datasetargument.Note:it'spossiblethattherewill
benomatchingpatternsinthemodeltoapplyAWQ,in
suchcaseitwillbeskipped.
--scale-estimationIndicateswhethertoapplyascaleestimation
algorithmthatminimizestheL2errorbetweenthe
originalandcompressedlayers.Providingadatasetis
requiredtorunscaleestimation.Pleasenote,that
applyingscaleestimationtakesadditionalmemoryand
time.
--sensitivity-metricSENSITIVITY_METRIC
Thesensitivitymetricforassigningquantization
precisiontolayers.Canbeoneofthefollowing:
['weight_quantization_error',
'hessian_input_activation',
'mean_activation_variance','max_activation_variance',
'mean_activation_magnitude'].
--num-samplesNUM_SAMPLES
Themaximumnumberofsamplestotakefromthedataset
forquantization.
--disable-statefulDisablestatefulconvertedmodels,statelessmodels
willbegeneratedinstead.Statefulmodelsare
producedbydefaultwhenthiskeyisnotused.In
statefulmodelsallkv-cacheinputsandoutputsare
hiddeninthemodelandarenotexposedasmodel
inputsandoutputs.If--disable-statefuloptionis
used,itmayresultinsub-optimalinference
performance.Useitwhenyouintentionallywanttouse
astatelessmodel,forexample,tobecompatiblewith
existingOpenVINOnativeinferencecodethatexpects
kv-cacheinputsandoutputsinthemodel.
--disable-convert-tokenizer
Donotaddconvertedtokenizeranddetokenizer
OpenVINOmodels.
--fp16Compressweightstofp16
--int8Compressweightstoint8
--convert-tokenizer[Deprecated]Addconvertedtokenizeranddetokenizer
withOpenVINOTokenizers.


ThecommandlineexportformodelfromexampleabovewithFP16weights
compression:

..code::ipython3

!optimum-cliexportopenvino--model$MODEL--tasktext-classification--fp16models/optimum_model/fp16


..parsed-literal::

huggingface/tokenizers:Thecurrentprocessjustgotforked,afterparallelismhasalreadybeenused.Disablingparallelismtoavoiddeadlocks...
Todisablethiswarning,youcaneither:
	-Avoidusing`tokenizers`beforetheforkifpossible
	-ExplicitlysettheenvironmentvariableTOKENIZERS_PARALLELISM=(true|false)


..parsed-literal::

2024-07-1300:35:45.994137:Wtensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38]TF-TRTWarning:CouldnotfindTensorRT
/opt/home/k8sworker/ci-ai/cibuilds/ov-notebook/OVNotebookOps-727/.workspace/scm/ov-notebook/.venv/lib/python3.8/site-packages/diffusers/utils/outputs.py:63:UserWarning:torch.utils._pytree._register_pytree_nodeisdeprecated.Pleaseusetorch.utils._pytree.register_pytree_nodeinstead.
torch.utils._pytree._register_pytree_node(
`--fp16`optionisdeprecatedandwillberemovedinafutureversion.Use`--weight-format`instead.
Frameworknotspecified.Usingpttoexportthemodel.
Someweightsofthemodelcheckpointatcardiffnlp/twitter-roberta-base-sentiment-latestwerenotusedwheninitializingRobertaForSequenceClassification:['roberta.pooler.dense.bias','roberta.pooler.dense.weight']
-ThisISexpectedifyouareinitializingRobertaForSequenceClassificationfromthecheckpointofamodeltrainedonanothertaskorwithanotherarchitecture(e.g.initializingaBertForSequenceClassificationmodelfromaBertForPreTrainingmodel).
-ThisISNOTexpectedifyouareinitializingRobertaForSequenceClassificationfromthecheckpointofamodelthatyouexpecttobeexactlyidentical(initializingaBertForSequenceClassificationmodelfromaBertForSequenceClassificationmodel).
UsingframeworkPyTorch:2.3.1+cpu
Overriding1configurationitem(s)
	-use_cache->False
OpenVINOTokenizersisnotavailable.TodeploymodelsinproductionwithC++code,pleasefollowinstallationinstructions:https://github.com/openvinotoolkit/openvino_tokenizers?tab=readme-ov-file#installation

Tokenizerwon'tbeconverted.


Afterexport,modelwillbeavailableinthespecifieddirectoryandcan
beloadedusingthesameOVModelForXXXclass.

..code::ipython3

model=OVModelForSequenceClassification.from_pretrained("models/optimum_model/fp16",device=device.value)


..parsed-literal::

CompilingthemodeltoAUTO...


TherearesomemodelsintheHuggingFaceModelsHub,thatarealready
convertedandreadytorun!Youcanfilterthosemodelsoutbylibrary
name,justtypeOpenVINO,orfollow`this
link<https://huggingface.co/models?library=openvino&sort=trending>`__.

TheOptimumModelInference
~~~~~~~~~~~~~~~~~~~~~~~~~~~

`backtotop‚¨ÜÔ∏è<#table-of-contents>`__

Modelinferenceisexactlythesameasfortheoriginalmodel!

..code::ipython3

output=model(**encoded_input)
scores=output[0][0]
scores=torch.softmax(scores,dim=0).numpy(force=True)

print_prediction(scores)


..parsed-literal::

1)positive0.9483
2)neutral0.0485
3)negative0.0031


YoucanfindmoreexamplesofusingOptimumIntelhere:1.`Accelerate
InferenceofSparseTransformer
Models<sparsity-optimization-with-output.html>`__2.
`GrammaticalErrorCorrectionwith
OpenVINO<grammar-correction-with-output.html>`__3.`Stable
Diffusionv2.1usingOptimum-Intel
OpenVINO<stable-diffusion-v2-with-output.html>`__
4.`ImagegenerationwithStableDiffusion
XL<../stable-diffusion-xl>`__5.`Instructionfollowingusing
DatabricksDolly2.0<../dolly-2-instruction-following>`__6.`Create
LLM-poweredChatbotusingOpenVINO<../llm-chatbot>`__7.`Document
VisualQuestionAnsweringUsingPix2Structand
OpenVINO<../pix2struct-docvqa>`__8.`Automaticspeechrecognition
usingDistil-WhisperandOpenVINO<../distil-whisper-asr>`__
